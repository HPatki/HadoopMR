For anyone paying attention to the Map and Reduce classes, it will become clear that Hadoop processes data based on Key's. More often that not, we tend to imagine input files to contain 'record's', one per line. This is a very simplistic view of looking at input data. However, input data can be more complex than this. Data can be binary in form of PDF files, word documents, Excel Files et all.

Hadoop can process such data with a little help provided by the developer. As an example, let's assume we are going to process PDF files.
The PDF file does not contain 'individual records'. Rather, it could be 'one' record itself. Thus each file becomes one record. As we know, one record is processed by one mapper. Thus one PDF file would be processed by one mapper - it will not get split across multiple mappers. Please note, however, that there will be multiple PDF files for processing. And Hadoop may opt to start multiple mappers to process multiple PDF files.

Also, for efficiency, it makes sense to combine multiple files into one file. Having done so, combined file would then have multiple 'records' in it. Such a file is 'splittable' and can potentially get distributed across multiple mappers based on how the input splits happen.
